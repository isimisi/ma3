{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part I: RAG\n",
    "\n",
    "Please see the description of the assignment in the README file (section 1) <br>\n",
    "**Guide notebook**: [guides/rag_guide.ipynb](guides/rag_guide.ipynb)\n",
    "\n",
    "\n",
    "***\n",
    "<br>\n",
    "\n",
    "* Remember to include some reflections on your results. Are there, for example, any hyperparameters that are particularly important?\n",
    "\n",
    "* You should follow the steps given in the `rag_guide` notebook to create your own RAG system.\n",
    "\n",
    "<br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Any\n",
    "from copy import deepcopy\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from decouple import config\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import Image, display\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters.markdown import MarkdownHeaderTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ibm import WatsonxEmbeddings\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from langgraph.graph import START, StateGraph\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "import litellm\n",
    "from litellm import completion\n",
    "import instructor\n",
    "from instructor import Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "WX_API_KEY = config(\"WX_API_KEY\")\n",
    "WX_PROJECT_ID = config(\"WX_PROJECT_ID\")\n",
    "WX_API_URL = \"https://us-south.ml.cloud.ibm.com\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authenticate and initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = WatsonxLLM(\n",
    "\n",
    "        model_id= \"ibm/granite-3-8b-instruct\",\n",
    "        url=WX_API_URL,\n",
    "        apikey=WX_API_KEY,\n",
    "        project_id=WX_PROJECT_ID,\n",
    "\n",
    "        params={\n",
    "            GenParams.DECODING_METHOD: \"greedy\",\n",
    "            GenParams.TEMPERATURE: 0,\n",
    "            GenParams.MIN_NEW_TOKENS: 5,\n",
    "            GenParams.MAX_NEW_TOKENS: 1_000,\n",
    "            GenParams.REPETITION_PENALTY:1.2\n",
    "        }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m document = \u001b[43mTextLoader\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mdata/madeup_company.md\u001b[39m\u001b[33m'\u001b[39m).load()[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'TextLoader' is not defined"
     ]
    }
   ],
   "source": [
    "document = TextLoader('data/madeup_company.md').load()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [(\"#\", \"Header 1\"), (\"##\", \"Header 2\"), (\"###\", \"Header 3\"), (\"####\", \"Header 4\")]\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "chunks = text_splitter.split_text(document.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_documents_with_headers(chunks):\n",
    "    \"\"\"\n",
    "    Creates a new list of Document objects with page_content prepended with headers\n",
    "    in [Header1/Header2/Header3]: format\n",
    "    \n",
    "    Returns new objects rather than modifying the original chunks\n",
    "    \"\"\"\n",
    "    updated_chunks = []\n",
    "    \n",
    "    for doc in chunks:\n",
    "        # Create a deep copy of the document to avoid modifying the original\n",
    "        new_doc = deepcopy(doc)\n",
    "        \n",
    "        # Get all headers that exist in metadata\n",
    "        headers = []\n",
    "        for i in range(1, 4):\n",
    "            key = f'Header {i}'\n",
    "            if key in new_doc.metadata:\n",
    "                headers.append(new_doc.metadata[key])\n",
    "        \n",
    "        # Create the header prefix and update page_content\n",
    "        if headers:\n",
    "            prefix = f\"[{'/'.join(headers)}]: \"\n",
    "            new_doc.page_content = prefix + \"\\n\" + new_doc.page_content\n",
    "        \n",
    "        updated_chunks.append(new_doc)\n",
    "    \n",
    "    return updated_chunks\n",
    "\n",
    "\n",
    "docs = update_documents_with_headers(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_params = {}\n",
    "\n",
    "watsonx_embedding = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/granite-embedding-278m-multilingual\",\n",
    "    url=WX_API_URL,\n",
    "    project_id=WX_PROJECT_ID,\n",
    "    apikey=WX_API_KEY,\n",
    "    params=embed_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_vector_db = Chroma.from_documents(\n",
    "    collection_name=\"my_collection\",\n",
    "    embedding=watsonx_embedding,\n",
    "    persist_directory=\"my_vector_db\", # This will save the vector database to disk! Delete it if you want to start fresh.\n",
    "    documents=docs,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context: \n",
    "{context} \n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    \"\"\" A langgraph state for the application \"\"\"\n",
    "    question: str\n",
    "    context: list[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    \"\"\" Our retrieval step. We use our local vector database to retrieve similar documents to the question \"\"\"\n",
    "    retrieved_docs = local_vector_db.similarity_search(state[\"question\"], k=3) # NOTE: You can change k to retrieve fewer or more documents\n",
    "    return {\"context\": retrieved_docs} \n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    \"\"\" Our generation step. We use the retrieved documents to generate an answer to the question \"\"\"\n",
    "\n",
    "    # Format the prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    formated_prompt = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "\n",
    "    # Generate the answer\n",
    "    response = llm.invoke(formated_prompt)\n",
    "    return {\"answer\": response}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\") # Start at the retrieve step\n",
    "graph = graph_builder.compile() # Compile the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What's the roadmap ahead?\",\n",
       " 'context': [Document(id='64e71e93-2cb6-4ade-9592-99fc2dc1d887', metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Roadmap'}, page_content=\"[About MadeUpCompany/Roadmap]: \\nWe are constantly evolving and introducing new features based on customer feedback. Hereâ€™s whatâ€™s coming soon:  \\n- ðŸš€ AI-Driven Data Insights â€“ DataWiz will introduce automated trend forecasting powered by deep learning.\\n- ðŸš€ Collaboration Tools for CloudMate â€“ Enhanced real-time document editing and team workspaces for seamless collaboration.\\n- ðŸš€ Zero-Knowledge Encryption â€“ An optional feature for businesses requiring absolute data confidentiality.  \\nWe value our customers' input and prioritize updates that deliver the most impact.\"),\n",
       "  Document(id='f56bb01c-058b-4466-ad1c-8f553ff3215d', metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Our Values'}, page_content=\"[About MadeUpCompany/Our Values]: \\nAt MadeUpCompany, we believe in:  \\n- Innovation â€“ Continuously developing and refining solutions that meet the evolving needs of businesses.\\n- Security & Privacy â€“ Implementing world-class security protocols to protect our customers' data.\\n- Customer-Centric Approach â€“ Designing intuitive, powerful tools that make complex technology accessible.\\n- Sustainability â€“ Ensuring our infrastructure is energy-efficient and environmentally responsible.\"),\n",
       "  Document(id='0287a521-c903-485d-81c6-c4f05be23eed', metadata={'Header 1': 'About MadeUpCompany'}, page_content='[About MadeUpCompany]: \\nMadeUpCompany is a pioneering technology firm founded in 2010, specializing in cloud computing, data analytics, and machine learning. Headquartered in San Francisco, California, we have a global presence with satellite offices in New York, London, and Tokyo. Our mission is to empower businesses and individuals with cutting-edge technology that enhances efficiency, scalability, and innovation.  \\nWith a diverse team of experts from various industriesâ€”including AI research, cybersecurity, and enterprise software developmentâ€”we push the boundaries of whatâ€™s possible. Our commitment to continuous improvement, security, and customer success has earned us recognition as a leader in the tech space.')],\n",
       " 'answer': 'The upcoming features include AI-driven data insights (DataWiz), enhanced collaboration tools for CloudMate, and zero-knowledge encryption. These developments align with our values of innovation, security, and customer centricity.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = graph.invoke({ \"question\": \"What's the roadmap ahead?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The upcoming features include AI-driven data insights (DataWiz), enhanced collaboration tools for CloudMate, and zero-knowledge encryption. These developments align with our values of innovation, security, and customer centricity.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.get('answer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm gonna be honest here and say that I found it more useful to spend my time on the final exam.\n",
    "\n",
    "I have read the guides and understand the conecpts of RAG, Agents and using LLM's to evaluate. \n",
    "\n",
    "With RAG we are able to minimise halucinations by grounding the model in some source of truth. This is useful when we want to know factual information about some topic, but don't want to read an entire book on the matter. By embedding text into a vector database we can retrieve information based on the user's query, to feed into the LLM along with the user query, and let the LLM figure out how to respond to the query, based on the what's being asked and the information retrieved by the vector db. \n",
    "\n",
    "- We may tune a RAG model by adjusting the way we preprocess documents. How do we get access to the data plays an important role, and how the data looks (e.g., if we scrape websites, can we be sure that they are structured in the same way?). This is important as we want to split up the documents into meaningful passages, small enough to get the information needed to answer questions, but not so small that it beccomes too specific. We also don't want the split to be too large, as that will trigger more halucinations by the LLM. Finding a good way to split the document can be very hard, but also a good way to improve a RAG model. \n",
    "\n",
    "- The embedding an retrieval process of the RAG model also plays a role, but this is highly optimised by open source projects, like elastic-search. How many splits we give the LLM can also be tuned, based either on the top k results or the p percentage.\n",
    "\n",
    "- The way we prompt the model will also play a large role in optimisation.\n",
    "\n",
    "Using LLM's to evaluate is possibly the only way to effectively evalutate such atonomous systems, as the response given requires one to understand the output, and tell if it's actually usefull. This kind of works like the `split brain theory`, where one hemisphere generates a response while the other interprets and rationalizes it.\n",
    "\n",
    "---\n",
    "\n",
    "**Agents** are usefull in creating larger autonomous systems where we allow the LLM to make decisions for you. The Router agent is useful for creating a complete task flow, that is often tailored to a specific need, where the tool agent more atonomously decides what to do next. The router agent is connected by edges defined by the programmer, and the tool agent just has a set of tools where the LLM decides which tools to use.\n",
    "\n",
    "---\n",
    "\n",
    "One project that came to mind while reading these guides was to build an agent that allows you to gather information on a book based on the [GoodReads](https://www.goodreads.com/) reviews. Every site has the same interface, so it would be fairly easy to scrape the website.\n",
    "\n",
    "**steps**\n",
    "1. Get request from user\n",
    "2. find title, author and year of book\n",
    "3. check if book exists in vector db\n",
    "3. (a: book exists) get top k result from vector db, and answer query (END)\n",
    "4. (b: book doesnt exist) search on goodreads for book, and scrape top result\n",
    "5. split content based on reviews, and maybe subsplit reviews if they are too long\n",
    "6. embed results and save to vector db\n",
    "7. get top k result from vector db, and answer query (END)\n",
    "\n",
    "possibly with more steps, but that's the idea of it\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
